{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2064,
     "status": "ok",
     "timestamp": 1622443062000,
     "user": {
      "displayName": "Marchete",
      "photoUrl": "",
      "userId": "01"
     },
     "user_tz": -120
    },
    "id": "xxxxIooooobK",
    "outputId": "91ab953e-8c83-4a48-b029-00000a1261ac"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "THREADS=3\n",
    "tf.config.threading.set_intra_op_parallelism_threads(THREADS)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(THREADS)\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import base64\n",
    "import sys\n",
    "import glob, os\n",
    "import re\n",
    "import subprocess\n",
    "import datetime\n",
    "import queue\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\"Python:\"+sys.version)\n",
    "print(\"TF:\"+tf.__version__)\n",
    "print(\"GPU:\"+str(tf.test.is_gpu_available())+\" CUDA:\"+str(tf.test.is_built_with_cuda()))\n",
    "#print(device_lib.list_local_devices())\n",
    "import os\n",
    "if not os.path.exists('/run/shm/traindata'):\n",
    "    os.makedirs('/run/shm/traindata')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crkTUlVwJBbO"
   },
   "outputs": [],
   "source": [
    "MATCHES_PER_GENERATION=800\n",
    "PIT_MATCHES=400\n",
    "THREADS=4\n",
    "#G7_TRAINING_POOL_SIZE=800000\n",
    "#TRAINING_POOL_SIZE=600000\n",
    "TRAINING_POOL_SIZE=3500000  #gen 157 suffering low diversity\n",
    "K_BATCH_SIZE=32\n",
    "K_ITERATIONS=650\n",
    "K_EPOCH=20\n",
    "#G7_TRAINING_SUBSET_SIZE=600000\n",
    "TRAINING_SUBSET_SIZE=350000\n",
    "K_WEIGHT_POLICY=1.0\n",
    "K_WEIGHT_VALUE=1.0\n",
    "TRAINING_LOOPS=50\n",
    "WINRATE_ACCEPTED=55.6\n",
    "INPUT_SIZE=6*2*24+2*27\n",
    "POLICY_SIZE=6\n",
    "\n",
    "LEARNING_RATE_MIN=0.00005\n",
    "LEARNING_RATE_MAX=0.00021\n",
    "#K_LEARNING_RATE=0.00007\n",
    "#G7_K_LEARNING_RATE=0.005\n",
    "K_LEARNING_RATE=0.001\n",
    "#up to gen9 K_LEARNING_RATE=0.0007 \n",
    "#K_LEARNING_RATE=0.00009 #gen 10 \n",
    "\n",
    "PROPAGATE_BASE=0.90 #f; //Propagate \"WIN/LOSS\" with 10% at start\n",
    "PROPAGATE_INC=0.10#1.0-PROPAGATE_BASE #f; //Linearly increase  until endgame\n",
    "#PROPAGATE_BASE=0.20 #f; //Propagate \"WIN/LOSS\" with 10% at start\n",
    "#PROPAGATE_INC=1.0-PROPAGATE_BASE #f; //Linearly increase  until endgame\n",
    "POLICY_BACKP_FIRST=20 #; //Similarly , but with percentage of turns, first 30% of turns doesn't have any \"temperature\",\n",
    "POLICY_BACKP_LAST=5 #; //from 30% to (100-10=90%) I linearly sharpen policy to get only the best move, \n",
    "\n",
    "K_L2=0.00002\n",
    "TRAIN_CPUCT_MIN=3.00\n",
    "TRAIN_CPUCT_INC=0.00\n",
    "TRAIN_CPUCT_MAX=3.00\n",
    "#gen136\n",
    "TRAIN_CPUCT_MIN=2.8965516739437245\n",
    "TRAIN_CPUCT_INC=0.0034086042277349836 \n",
    "TRAIN_CPUCT_MAX=2.974563702328649\n",
    "\n",
    "TRAIN_NOISE_DIR_EPSILON=0.20\n",
    "#TRAIN_NOISE_DIR_ALPHA=1.3\n",
    "#TRAIN_NOISE_DIR_DECAY=0.0007\n",
    "#TRAIN_NOISE_DIR_DECAY=0.0002 #gen 136\n",
    "TRAIN_NOISE_DIR_ALPHA=1.6 #gen 154\n",
    "TRAIN_NOISE_DIR_DECAY=0.0 #gen 154\n",
    "TRAIN_NOISE_DIR_EPSILON=0.24 #gen 174\n",
    "TRAIN_NOISE_DIR_ALPHA=1.5 #gen 174\n",
    "TRAIN_NOISE_DIR_DECAY=0.0 #gen 174\n",
    "\n",
    "TRAIN_MCTS_ITER=2000\n",
    "TRAIN_NOISE_RANDOM=0.0\n",
    "TRAIN_NOISE_RANDOM=0.05 #gen 174\n",
    "TRAIN_PARAMS = f\"{TRAIN_CPUCT_MIN} {TRAIN_CPUCT_INC} {TRAIN_CPUCT_MAX} {TRAIN_MCTS_ITER} {TRAIN_NOISE_DIR_EPSILON} {TRAIN_NOISE_DIR_ALPHA} {TRAIN_NOISE_DIR_DECAY} {TRAIN_NOISE_RANDOM} {PROPAGATE_BASE} {PROPAGATE_INC} {POLICY_BACKP_FIRST} {POLICY_BACKP_LAST}\"\n",
    "\n",
    "PIT_CPUCT_MIN=3.00\n",
    "PIT_CPUCT_INC=0.00\n",
    "PIT_CPUCT_MAX=3.00\n",
    "#gen136\n",
    "PIT_CPUCT_MIN=2.8965516739437245\n",
    "PIT_CPUCT_INC=0.0034086042277349836 \n",
    "PIT_CPUCT_MAX=2.974563702328649\n",
    "\n",
    "#PIT_NOISE_DIR_EPSILON=0.04 \n",
    "PIT_NOISE_DIR_EPSILON=0.04  #gen 174\n",
    "PIT_NOISE_DIR_ALPHA=1.3\n",
    "PIT_NOISE_DIR_DECAY=0.0007\n",
    "PIT_MCTS_ITER=3000\n",
    "PIT_NOISE_RANDOM=0.02  #gen 174\n",
    "PIT_PARAM_THREAD= f\"{THREADS} {PIT_MATCHES}\"\n",
    "PIT_PARAM_MCTS = f\"{PIT_CPUCT_MIN} {PIT_CPUCT_INC} {PIT_CPUCT_MAX} {PIT_MCTS_ITER} {PIT_NOISE_DIR_EPSILON} {PIT_NOISE_DIR_ALPHA} {PIT_NOISE_DIR_DECAY} {PIT_NOISE_RANDOM} {PROPAGATE_BASE} {PROPAGATE_INC} {POLICY_BACKP_FIRST} {POLICY_BACKP_LAST}\"\n",
    "SAMPLES_FILE=os.path.join(\".\",\"traindata\",\"samples.dat\")\n",
    "sampler_process=os.path.join(\".\",\"NNSampler\")+\" \"+os.path.join(\".\",\"traindata\")+\" Replay.*.dat \"+SAMPLES_FILE+\" \"+str(TRAINING_POOL_SIZE)+\" \"+str(TRAINING_SUBSET_SIZE)+\" \"+str(INPUT_SIZE)+\" \"+str(1+POLICY_SIZE)+\" 1\"\n",
    "print(LEARNING_RATE_MAX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjustLR():\n",
    "    if generation == 0:\n",
    "        return 0.008\n",
    "    elif generation < 5:\n",
    "        return 0.006\n",
    "    elif generation < 10:\n",
    "        return 0.002\n",
    "    else:\n",
    "        return 0.0006\n",
    "def adjustLR2():\n",
    "    if generation == 0:\n",
    "        return 0.005\n",
    "    elif generation < 5:\n",
    "        return 0.001\n",
    "    elif generation < 10:\n",
    "        return 0.0007 \n",
    "    else:\n",
    "        return 0.00008\n",
    "\n",
    "def policy_loss(y_true, y_pred): \n",
    "    p_true = tf.where(y_true >= 0.0, y_true, 0)\n",
    "    p_pred = tf.where(y_true >= 0.0, y_pred, -99999999.99)\n",
    "    p_pred = tf.nn.softmax(p_pred)\n",
    "    #this is wrong, categorical_crossentropy should be tf.keras.losses.KLD\n",
    "    return 0.8*tf.keras.losses.categorical_crossentropy(p_true, p_pred)+0.2*tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "def learning_rate_scheduler(epoch, lr):\n",
    "    if (generation<4):\n",
    "        return 0.02-0.003*generation\n",
    "    else:\n",
    "        LR_RANGE=LEARNING_RATE_MAX-LEARNING_RATE_MIN\n",
    "        MID_EPOCH=K_ITERATIONS*0.5\n",
    "        LR_STEP=LR_RANGE/MID_EPOCH        \n",
    "        DIST_EPOCH=abs(MID_EPOCH-epoch)\n",
    "        L = LEARNING_RATE_MAX -LR_STEP*DIST_EPOCH\n",
    "        if (generation<15):\n",
    "            return max(L,LEARNING_RATE_MIN)*2.0\n",
    "        else:\n",
    "            return min(max(L,LEARNING_RATE_MIN),LEARNING_RATE_MAX)\n",
    "def SaveModel(my_model,fileSTR):\n",
    "    totalbytes=0\n",
    "    data=[]\n",
    "    Wmodel = open(\"./\"+fileSTR, \"wb\")\n",
    "    for x in my_model.weights:\n",
    "        nn = x.numpy()\n",
    "        T = nn\n",
    "        v = np.ndarray.tobytes(T)\n",
    "        Wmodel.write(bytearray(v))\n",
    "        totalbytes+=len(v)\n",
    "        data.append(base64.b64encode(v).decode(\"utf-8\"))\n",
    "    Wmodel.close()\n",
    "def readWinrate(candidatefile,bestfile):\n",
    "    files=sorted(glob.glob(os.path.join('pitresults','Pit_'+candidatefile+'_'+bestfile+'_*.txt')),reverse=True)\n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            strvalue=f.read().strip()\n",
    "        return float(strvalue)\n",
    "    return -1.0\n",
    "def readAndDeleteWinrate(candidatefile,bestfile):\n",
    "    files=sorted(glob.glob(os.path.join('pitresults','Pit_'+candidatefile+'_'+bestfile+'_*.txt')),reverse=True)\n",
    "    valor=-1.0\n",
    "    for file in files:\n",
    "        if (valor == -1.0):\n",
    "            with open(file, 'r') as f:\n",
    "                valor=float(f.read().strip())\n",
    "        os.remove(file)\n",
    "    return valor\n",
    "\n",
    "LR_callback = tf.keras.callbacks.LearningRateScheduler(learning_rate_scheduler)\n",
    "LR_decay = tf.keras.callbacks.LearningRateScheduler(tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-3,decay_steps=20000,decay_rate=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEr5GjiJJBbP"
   },
   "outputs": [],
   "source": [
    "inputs =  tf.keras.Input(shape=(INPUT_SIZE,), name='input')\n",
    "x = tf.keras.layers.Dense(TODOTODOTODO,activation='relu',name='Dense1')(inputs)\n",
    "#x = tf.keras.layers.Dense(TODOTODOTODO,activation='relu')(x)\n",
    "\n",
    "p1 = tf.keras.layers.Dense(TODOTODOTODO,activation='relu',name='p1')(x)\n",
    "#p1 = tf.keras.layers.Dense(TODOTODOTODO,activation='relu')(p1)\n",
    "v1 = tf.keras.layers.Dense(TODOTODOTODO,activation='relu',name='v1')(x)\n",
    "#v1 = tf.keras.layers.Dense(TODOTODOTODO,activation='relu')(v1)\n",
    "value = tf.keras.layers.Dense(1, activation='tanh',name='value')(v1)\n",
    "policy = tf.keras.layers.Dense(POLICY_SIZE, activation='softmax',name='policy')(p1)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=[value, policy])\n",
    "\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate=K_LEARNING_RATE)\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=K_LEARNING_RATE)\n",
    "csv_logger = tf.keras.callbacks.CSVLogger('gen_train.log',append=True)\n",
    "\n",
    "model.compile(loss={'value': 'mean_squared_error',\n",
    "                    #'policy': policy_loss\n",
    "                    #'policy':'categorical_crossentropy'\n",
    "                    'policy':tf.keras.losses.KLD\n",
    "                   },\n",
    "              optimizer=opt,\n",
    "              loss_weights = {'value':K_WEIGHT_VALUE,\n",
    "                              'policy':K_WEIGHT_POLICY}\n",
    "              #,steps_per_execution=TRAINING_LOOPS,\n",
    "              ,metrics={'value':'mean_absolute_percentage_error',\n",
    "                        #'value':'mean_absolute_error',\n",
    "                       'policy': 'categorical_accuracy' }\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kbz3qqCMJBbQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('gen0000.h5'):\n",
    "    model.save('gen0000.h5')\n",
    "    SaveModel(model,\"gen0000.w32\")\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1I1ex_D8JBbS",
    "outputId": "fd3d4b2c-00dc-44a2-d88b-77ba3f51b487"
   },
   "outputs": [],
   "source": [
    "generation=0\n",
    "gen_name=\"gen\"+str(generation).zfill(4)\n",
    "if os.path.exists('generation.txt'):\n",
    "    with open('generation.txt', 'r') as f:\n",
    "        generation = int(f.read().strip())\n",
    "        gen_name=\"gen\"+str(generation).zfill(4)\n",
    "        print(\"Generation is :\"+gen_name+\" \"+str(generation))        \n",
    "        if (generation > 0):\n",
    "            model =  tf.keras.models.load_model(gen_name+'.h5', custom_objects={\"policy_loss\": policy_loss})\n",
    "if not os.path.exists(gen_name+\".w32\"):\n",
    "    SaveModel(model,gen_name+\".w32\")\n",
    "    model.save(gen_name+'.h5')\n",
    "gen_best1=gen_name\n",
    "if os.path.exists('gen_best1.txt'):\n",
    "    with open('gen_best1.txt', 'r') as f:\n",
    "        gen_best1 = f.read().strip()\n",
    "gen_best2=gen_name\n",
    "if os.path.exists('gen_best2.txt'):\n",
    "    with open('gen_best2.txt', 'r') as f:\n",
    "        gen_best2 = f.read().strip()        \n",
    "model=tf.keras.models.load_model(gen_name+'.h5', custom_objects={\"policy_loss\": policy_loss})\n",
    "print(\"Best Model1:\"+gen_best1+\" + \"+gen_best2+\". Current generation:\"+gen_name+\" loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zULZ7RjSJBbT",
    "outputId": "93c0f940-cecf-41cf-ae95-e337af017d60",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clear_output(wait=True)\n",
    "try:\n",
    "    pit_winrate2\n",
    "except NameError:\n",
    "    pit_winrate2=70.0\n",
    "try:\n",
    "    pit_winrate\n",
    "except NameError:\n",
    "    pit_winrate=max(0,100.0-pit_winrate2)\n",
    "\n",
    "pit_winrate2=70\n",
    "pit_winrate=70\n",
    "    \n",
    "while True:\n",
    "    model.optimizer.learning_rate.assign(adjustLR())\n",
    "    if (generation % 10 == 0):\n",
    "        clear_output(wait=True)\n",
    "    samplescount=0\n",
    "    #if (generation == 256):\n",
    "     #   csv_data = np.fromfile(SAMPLES_FILE, dtype=np.float32)\n",
    "      #  csv_data=np.reshape(csv_data, (-1,INPUT_SIZE+POLICY_SIZE+2))\n",
    "       # samplescount =(csv_data.shape)[0]\n",
    "    #generate enough diversity to train\n",
    "    while samplescount < TRAINING_SUBSET_SIZE/3:\n",
    "        if (generation==0):\n",
    "            random_enemy=gen_name\n",
    "        else:\n",
    "            random_enemy=\"gen\"+str(random.randint(max(1,generation-5), generation)).zfill(4)\n",
    "        randpick=random.randint(0,100)\n",
    "\n",
    "        if (generation==0 and (len(glob.glob(os.path.join('traindata','Replay_'+'*'+gen_name+\"vs\"+gen_name+'.dat')))>0)):\n",
    "            print('Replay_'+'*'+gen_name+'.txt already exists') \n",
    "        else:\n",
    "            pFirst=max(0.2,0.9*(1.0-(pit_winrate/(pit_winrate+pit_winrate2))))\n",
    "            pSecond=0.9-pFirst\n",
    "            print(f\" **** Doing samples. Count:{samplescount}. pBest1:{100.0*pFirst}% p2:{100.0*pSecond}%  {pit_winrate} {pit_winrate2}\")\n",
    "            p70=int(pFirst*MATCHES_PER_GENERATION)\n",
    "            p20=int(pSecond*MATCHES_PER_GENERATION)\n",
    "            p5=MATCHES_PER_GENERATION-p70-p20\n",
    "            selfplay_process=os.path.join(\".\",\"CGZero\")+\" selfplay \"+f\"{THREADS} {p70} \"+gen_best1+\" \"+TRAIN_PARAMS+\" \"+gen_best1+\" \"+TRAIN_PARAMS\n",
    "            print(selfplay_process)\n",
    "            subprocess.run(selfplay_process, shell=True)\n",
    "            selfplay_process=os.path.join(\".\",\"CGZero\")+\" selfplay \"+f\"{THREADS} {p20} \"+gen_best1+\" \"+TRAIN_PARAMS+\" \"+gen_best2+\" \"+TRAIN_PARAMS\n",
    "            print(selfplay_process)\n",
    "            subprocess.run(selfplay_process, shell=True)\n",
    "            if (p70 >= p20):\n",
    "                A=gen_best1 if (gen_best1 >= random_enemy) else random_enemy\n",
    "                B=gen_best1 if (A == random_enemy) else random_enemy\n",
    "            else:\n",
    "                A=gen_best2 if (gen_best2 >= random_enemy) else random_enemy\n",
    "                B=gen_best2 if (A == random_enemy) else random_enemy\n",
    "            selfplay_process=os.path.join(\".\",\"CGZero\")+\" selfplay \"+f\"{THREADS} {p5} \"+A+\" \"+TRAIN_PARAMS+\" \"+B+\" \"+TRAIN_PARAMS\n",
    "            print(selfplay_process)\n",
    "            subprocess.run(selfplay_process, shell=True)\n",
    "        print('Reading training data')\n",
    "        print(sampler_process)\n",
    "        subprocess.run(sampler_process, shell=True)\n",
    "        csv_data = np.fromfile(SAMPLES_FILE, dtype=np.float32)\n",
    "        csv_data=np.reshape(csv_data, (-1,INPUT_SIZE+POLICY_SIZE+2))\n",
    "        samplescount =(csv_data.shape)[0]\n",
    "    np.random.shuffle(csv_data)\n",
    "    cut_index = [(csv_data.shape)[1]-POLICY_SIZE-2, (csv_data.shape)[1]-2,(csv_data.shape)[1]-1]\n",
    "    samples,policy,value,countVisits=np.split(csv_data, cut_index,axis=1)\n",
    "    \n",
    "    mask= np.where(policy < 0, -999999999.99, policy)\n",
    "    policy= np.where(policy < 0, 0, policy) \n",
    "  \n",
    "    #1000 minibatches\n",
    "    #for loop in range(K_ITERATIONS):\n",
    "     #   print(\"Batch \"+str(loop)+\":\",end='')\n",
    "      #  indices = np.random.choice(value.shape[0], K_BATCH_SIZE, replace=False)\n",
    "       # S=samples[indices]\n",
    "       # P=policy[indices]\n",
    "       # V=value[indices]\n",
    "    #    #model.optimizer.learning_rate.assign(learning_rate_scheduler(loop, 0.0))\n",
    "    #    #model.optimizer.learning_rate.assign(K_LEARNING_RATE)\n",
    "       # model.fit({'input':S}, {'policy': P, 'value':V},verbose=2, epochs=K_EPOCH,callbacks=[csv_logger],batch_size=int(K_BATCH_SIZE/4))\n",
    "    model.fit({'input':samples}, {'policy': policy, 'value':value},verbose=2, epochs=K_EPOCH,callbacks=[csv_logger],batch_size=int(K_BATCH_SIZE/4))    \n",
    "    print('New generation '+gen_name+' -> '+\"gen\"+str(generation+1).zfill(4))\n",
    "    #new generation\n",
    "    generation=generation+1\n",
    "    gen_name=\"gen\"+str(generation).zfill(4)\n",
    "    #Saving\n",
    "    print('Save Model '+gen_name+'.h5')\n",
    "    model.save(gen_name+'.h5')\n",
    "    SaveModel(model,gen_name+\".w32\")\n",
    "    with open('generation.txt', 'w') as f:\n",
    "        f.write(str(generation))    \n",
    "    #pit\n",
    "    pitplay_process=os.path.join(\".\",\"CGZero\")+\" pitplay \"+PIT_PARAM_THREAD+\" \"+gen_name+\" \"+PIT_PARAM_MCTS+\" \"+gen_best1+\" \"+PIT_PARAM_MCTS\n",
    "    print('subprocess.run('+pitplay_process+', shell=True)')\n",
    "    subprocess.run(pitplay_process, shell=True)\n",
    "    #clear_output(wait=True)\n",
    "    #read pit value\n",
    "    pit_winrate=readWinrate(gen_name,gen_best1)\n",
    "    pitplay_process=os.path.join(\".\",\"CGZero\")+\" pitplay \"+PIT_PARAM_THREAD+\" \"+gen_name+\" \"+PIT_PARAM_MCTS+\" \"+gen_best2+\" \"+PIT_PARAM_MCTS\n",
    "    subprocess.run(pitplay_process, shell=True)\n",
    "    pit_winrate2=readWinrate(gen_name,gen_best2)\n",
    "    print('Winrate '+str(pit_winrate)+' '+str(pit_winrate2))\n",
    "    if (pit_winrate>=WINRATE_ACCEPTED):\n",
    "        print(\"New best:\"+gen_name+\" vs \"+gen_best1+\": Winrate:\"+str(pit_winrate)+\"%\")\n",
    "        #now vs best2\n",
    "        if gen_best1 != gen_best2:\n",
    "            print(\"        :\"+gen_name+\" vs \"+gen_best2+\": Winrate:\"+str(pit_winrate2)+\"%\")\n",
    "            tmpgenbest1=gen_best1\n",
    "            gen_best1=gen_name\n",
    "            with open('gen_best1.txt', 'w') as f:\n",
    "                f.write(gen_best1)\n",
    "        else:\n",
    "            pit_winrate2 = 49.0\n",
    "        if (pit_winrate2>=50.0):\n",
    "            gen_best2=tmpgenbest1\n",
    "            with open('gen_best2.txt', 'w') as f:\n",
    "                f.write(gen_best2)\n",
    "    #if (pit_winrate<=100.0-(WINRATE_ACCEPTED)):\n",
    "     #   print(\"WORST:\"+gen_name+\" vs \"+gen_best1+\": Winrate:\"+str(pit_winrate)+\"% Recover Best generation \"+gen_best1)\n",
    "      #  model=tf.keras.models.load_model(gen_best1+'.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(xs):\n",
    "    return np.exp(xs) / sum(np.exp(xs))\n",
    "\n",
    "xs = np.array([0.110082 ,0.129164 ,0.147099 ,0.136864 ,0.0711029 ,0.405688])\n",
    "print(softmax(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#SaveModel(model,\"prueba.w32\")\n",
    "#predictions = model.predict(samples)\n",
    "print(samples[8])\n",
    "print(\"Value Predicted:\",end='')\n",
    "print(predictions[0][8])\n",
    "\n",
    "print(\"Policy Predicted:\",end='')\n",
    "print(predictions[1][8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#mogen=tf.keras.models.load_model('gen0000.h5', custom_objects={\"policy_loss\": policy_loss})\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def lr2(epoch, lr):\n",
    "    return 0.02\n",
    "    #return 0.001\n",
    "    #return 0.001\n",
    "print(samples.shape)\n",
    "#print(mogen.optimizer.lr.read_value())\n",
    "mogen=model #tf.keras.models.load_model('gen0000.h5', custom_objects={\"policy_loss\": policy_loss})\n",
    "#mogen.optimizer.lr.assign(0.02)\n",
    "mogen.fit({'input':samples}, {'policy': policy, 'value': value},verbose=2, epochs=150,batch_size=64)\n",
    "#mogen.optimizer.lr.assign(0.005)\n",
    "mogen.fit({'input':samples}, {'policy': policy, 'value': value},verbose=2, epochs=150,batch_size=64)\n",
    "#mogen.optimizer.lr.assign(0.001)\n",
    "mogen.fit({'input':samples}, {'policy': policy, 'value': value},verbose=2, epochs=150,batch_size=64)\n",
    "#Adam loss: 0.8129 - value_loss: 0.1733 - policy_loss: 0.6396 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mogen.predict(inputs)\n",
    "print(len(policy))\n",
    "for n in range(30):\n",
    "    nv = n\n",
    "    print(str(nv)+\" Value Predicted:\"+str(abs(predictions[0][nv][0]-value[nv][0]))+\"  | \"+str(predictions[0][nv][0])+' '+str(value[nv][0]))\n",
    "    print(str(nv)+\" Policy Predicted:\",end='')\n",
    "    np.set_printoptions(suppress=True)\n",
    "    np.set_printoptions(precision=1)\n",
    "    print(predictions[1][nv],end='')\n",
    "    np.set_printoptions(precision=1)\n",
    "    print(policy[nv])\n",
    "    np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = np.fromfile('samples.dat', dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = np.fromfile(SAMPLES_FILE, dtype=np.float32)\n",
    "\n",
    "csv_data = np.fromfile('./traindata3/Replay_gen0256_gen0256.dat', dtype=np.float32)\n",
    "csv_data=np.reshape(csv_data, (-1,INPUT_SIZE+POLICY_SIZE+2))\n",
    "samplescount =(csv_data.shape)[0]\n",
    "cut_index = [(csv_data.shape)[1]-POLICY_SIZE-2, (csv_data.shape)[1]-2,(csv_data.shape)[1]-1]\n",
    "samples,policy,value,countVisits=np.split(csv_data, cut_index,axis=1)\n",
    "mask= np.where(policy < 0, -999999999.99, policy)\n",
    "policy= np.where(policy < 0, 0, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5)\n",
    "print(countVisits[0])\n",
    "print(policy[0])\n",
    "print(value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.learning_rate.assign(0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples.shape)\n",
    "print(policy.shape)\n",
    "print(value.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first20percent=int((samples.shape[0]*20/100))\n",
    "first40percent=int((samples.shape[0]*50/100))\n",
    "print(limit)\n",
    "print(  samples[:first20percent].shape)\n",
    "print(  samples[first20percent:first40percent].shape)\n",
    "print(  samples[first40percent:].shape)\n",
    "print(policy.shape)\n",
    "print(value.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training 20-40\")\n",
    "S40=samples[first20percent:first40percent]\n",
    "P40=policy[first20percent:first40percent]\n",
    "V40=value[first20percent:first40percent]     \n",
    "model.optimizer.lr.assign(0.0005)\n",
    "#model.fit({'input':S40}, {'policy': P40, 'value': V40},verbose=2, epochs=int(TRAINING_LOOPS*5/100),batch_size=256)        \n",
    "print(\"Training 20\")\n",
    "firstS=samples[:first20percent]\n",
    "firstP=policy[:first20percent]\n",
    "firstV=value[:first20percent]    \n",
    "if (generation < 3):\n",
    "    model.optimizer.lr.assign(0.01)\n",
    "else:        \n",
    "    model.optimizer.lr.assign(0.001)\n",
    "#model.fit({'input':firstS}, {'policy': firstP, 'value': firstV},verbose=2, epochs=int(TRAINING_LOOPS*10/100),batch_size=128)\n",
    "model.optimizer.lr.assign(0.0001)\n",
    "print(\"Running mini batches...\",end='')\n",
    "for r in range(int(TRAINING_LOOPS*85/100)):\n",
    "    #minibatch\n",
    "    selector=np.random.choice(value.shape[0], K_BATCH_SIZE, replace=False)\n",
    "    miniS=samples[selector]\n",
    "    miniP=policy[selector]\n",
    "    miniV=value[selector]\n",
    "    model.fit({'input':miniS}, {'policy': miniP, 'value': miniV},validation_data=({'input':firstS}, {'policy': firstP, 'value': firstV}),verbose=0, epochs=1)\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copyfile(gen_best1+'.w32', 'A.w32')\n",
    "shutil.copyfile(gen_best2+'.w32', 'B.w32')\n",
    "#cpupt optimizer\n",
    "BEST_CPUCT_MAX=1.00+random.random()*5.0    \n",
    "BEST_CPUCT_MIN=(BEST_CPUCT_MAX-0.40)*random.random()+0.40\n",
    "BEST_CPUCT_INC=0.001+0.035*random.random()\n",
    "TEST_PARAM_THREAD= f\"{THREADS} 150\"\n",
    "try:\n",
    "    bestWinrate\n",
    "except NameError:\n",
    "    bestWinrate=0.0\n",
    "with open('best_upct.txt', 'a') as f:\n",
    "    f.write(\"Searching on best:\"+gen_best1+\"\\n\")\n",
    "for upt in range(100):\n",
    "    TEST_CPUCT_INC=0.001+0.035*random.random()\n",
    "    TEST_CPUCT_MAX=1.00+random.random()*5.0    \n",
    "    TEST_CPUCT_MIN=(TEST_CPUCT_MAX-1.00)*random.random()+1.00    \n",
    "    if (random.random() < 0.5):\n",
    "        TEST_CPUCT_INC=-TEST_CPUCT_INC\n",
    "        T=TEST_CPUCT_MIN\n",
    "        TEST_CPUCT_MIN=TEST_CPUCT_MAX\n",
    "        TEST_CPUCT_MAX=T\n",
    "    #pit\n",
    "    #TEST_CPUCT_MAX=1.00+random.random()*5.0    \n",
    "    #TEST_CPUCT_MIN=(TEST_CPUCT_MAX-1.00)*random.random()+1.00\n",
    "    #TEST_CPUCT_INC=0.001+0.035*random.random()\n",
    "    TEST_CPUCT_MIN=3.0-0.30+2.0*0.30*random.random()\n",
    "    TEST_CPUCT_MAX=TEST_CPUCT_MIN+2.0*0.05*random.random()\n",
    "    TEST_CPUCT_INC=0.00360-0.00080+2.0*0.00080*random.random()\n",
    "    \n",
    "\n",
    "    UPT_CC = f\"{TEST_CPUCT_MIN} {TEST_CPUCT_INC} {TEST_CPUCT_MAX} {PIT_MCTS_ITER} {PIT_NOISE_DIR_EPSILON} {PIT_NOISE_DIR_ALPHA} {PIT_NOISE_DIR_DECAY} {PIT_NOISE_RANDOM} {PROPAGATE_BASE} {PROPAGATE_INC} {POLICY_BACKP_FIRST} {POLICY_BACKP_LAST}\"    \n",
    "    pitplay_process=os.path.join(\".\",\"CGZero\")+\" pitplay \"+TEST_PARAM_THREAD+\" A \"+UPT_CC+\" A \"+PIT_PARAM_MCTS\n",
    "    print('subprocess.run('+pitplay_process+', shell=True)')\n",
    "    subprocess.run(pitplay_process, shell=True)\n",
    "    pit_winrate=readAndDeleteWinrate(\"A\",\"A\")\n",
    "    if (pit_winrate > 50.0 and pit_winrate > bestWinrate-5.0):\n",
    "        pitplay_process=os.path.join(\".\",\"CGZero\")+\" pitplay \"+TEST_PARAM_THREAD+\" A \"+UPT_CC+\" B \"+PIT_PARAM_MCTS\n",
    "        subprocess.run(pitplay_process, shell=True)\n",
    "        pit_winrate+=readAndDeleteWinrate(\"A\",\"B\")\n",
    "        pit_winrate*=0.5\n",
    "    with open('best_upct.txt', 'a') as f:\n",
    "        f.write(f\"{pit_winrate} | {TEST_CPUCT_MIN} {TEST_CPUCT_INC} {TEST_CPUCT_MAX} \"+\"\\n\")    \n",
    "    print('Winrate '+str(pit_winrate))\n",
    "    if (pit_winrate > bestWinrate):\n",
    "        bestWinrate = pit_winrate\n",
    "        BEST_CPUCT_MAX=TEST_CPUCT_MAX   \n",
    "        BEST_CPUCT_MIN=TEST_CPUCT_MIN\n",
    "        BEST_CPUCT_INC=TEST_CPUCT_INC\n",
    "        print(f\"New best {TEST_CPUCT_MIN} {TEST_CPUCT_INC} {TEST_CPUCT_MAX} {bestWinrate}% \")\n",
    "        with open('best_upct.txt', 'a') as f:\n",
    "            f.write(f\"New best {bestWinrate}% | {TEST_CPUCT_MIN} {TEST_CPUCT_INC} {TEST_CPUCT_MAX}\"+\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = np.fromfile(SAMPLES_FILE, dtype=np.float32)\n",
    "csv_data=np.reshape(csv_data, (-1,INPUT_SIZE+POLICY_SIZE+2))\n",
    "cut_index = [(csv_data.shape)[1]-POLICY_SIZE-2, (csv_data.shape)[1]-2,(csv_data.shape)[1]-1]\n",
    "samples,policy,value=np.split(csv_data, cut_index,axis=1)\n",
    "\n",
    "mask= np.where(policy < 0, -999999999.99, policy)\n",
    "policy= np.where(policy < 0, 0, policy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples[-1])\n",
    "bestWinrate=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.learning_rate.assign(0.0005)\n",
    "model.fit({'input':samples}, {'policy': policy, 'value':value},verbose=2, epochs=int(60),callbacks=[csv_logger],batch_size=32)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SaveModel(model,\"superfit.w32\")\n",
    "VFF=1\n",
    "predictions = model.predict(samples)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(samples[VFF])\n",
    "print(\"Value Predicted:\",end='')\n",
    "print(\"pred:\"+str(predictions[0][VFF][0])+\" real:\"+str(value[VFF]))\n",
    "\n",
    "print(\"Policy Predicted:\",end='')\n",
    "print(predictions[1][VFF])\n",
    "print(policy[VFF])\n",
    "print(np.abs(policy[VFF]-predictions[1][VFF]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_pred=\",end=\"\")\n",
    "print(predictions[1][8])\n",
    "print(\" y_true=\",end=\"\")\n",
    "print(policy[8])\n",
    "print(\" diff=\",end=\"\")\n",
    "print( np.abs(policy[8]-predictions[1][8]))\n",
    "\n",
    "print(\" categorical_crossentropy=\",end=\"\")\n",
    "mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "mse(y_true, y_pred).numpy()\n",
    "print(tf.keras.losses.categorical_crossentropy(policy[8],predictions[1][8] ).numpy())\n",
    "\n",
    "print(\"y_pred=\",end=\"\")\n",
    "print(predictions[1][8])\n",
    "print(\" y_true=\",end=\"\")\n",
    "print(policy[8])\n",
    "print(\" KLD=\",end=\"\")\n",
    "print(tf.keras.losses.KLD(policy[8],predictions[1][8]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff=policy[8]- predictions[1][8]\n",
    "print(ff*ff)\n",
    "print(np.sum(ff*ff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.negative(tf.reduce_mean(tf.add(tf.multiply(policy[8],predictions[1][8]), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.multiply(policy[8],predictions[1][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=predictions[1][8]\n",
    "y_true=policy[8]\n",
    "print(\"y_pred=\",end=\"\")\n",
    "print(y_pred)\n",
    "print(\" y_true=\",end=\"\")\n",
    "print(y_true)\n",
    "H=tf.nn.MeanSquaredError(predictions[1][8],policy[8],reduction=\"mean\")\n",
    "print(H.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= np.array([0.0 ,0.0 , 0.0, 0.0,0.0, 1.0])\n",
    "y_true= np.array([1.0, 0.0 , 0.0, 0.0,0.0,0.0])\n",
    "print(tf.keras.losses.categorical_crossentropy(y_true,y_pred ).numpy())\n",
    "print(tf.keras.losses.binary_crossentropy(y_true,y_pred ).numpy())\n",
    "print(tf.keras.losses.KLD(y_true,y_pred ).numpy())\n",
    "print(\"EEEEEEEEEEEEE\")\n",
    "y_pred= np.array([0.5 ,0.0 , 0.0, 0.0,0.0, 0.5])\n",
    "y_true= np.array([1.0, 0.0 , 0.0, 0.0,0.0,0.0])\n",
    "print(tf.keras.losses.categorical_crossentropy(y_true,y_pred ).numpy())\n",
    "print(tf.keras.losses.binary_crossentropy(y_true,y_pred ).numpy())\n",
    "print(tf.keras.losses.KLD(y_true,y_pred ).numpy())\n",
    "print(\"EEEEEEEEEEEEE\")\n",
    "y_pred= np.array([0.45 ,0.45 , 0.0, 0.0,0.0, 0.10])\n",
    "y_true= np.array([0.5, 0.5 , 0.0, 0.0,0.0,0.0])\n",
    "print(tf.keras.losses.categorical_crossentropy(y_true,y_pred ).numpy())\n",
    "print(tf.keras.losses.binary_crossentropy(y_true,y_pred ).numpy())\n",
    "print(tf.keras.losses.KLD(y_true,y_pred ).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "mse(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subprocess.run('./NNSampler ./traindata3 Replay.*.dat ./traindata3/samples.dat 600000 400000 342 7 1', shell=True)\n",
    "csv_data=None\n",
    "samples=None\n",
    "policy=None\n",
    "value=None\n",
    "csv_data = np.fromfile('./traindata3/d1.dat', dtype=np.float32)\n",
    "csv_data=np.reshape(csv_data, (-1,INPUT_SIZE+POLICY_SIZE+2))\n",
    "cut_index = [(csv_data.shape)[1]-POLICY_SIZE-2, (csv_data.shape)[1]-2,(csv_data.shape)[1]-1]\n",
    "samples,policy,value=np.split(csv_data, cut_index,axis=1)\n",
    "mask=np.where(policy < 0, -999999999.99, policy)\n",
    "policy=np.where(policy < 0, 0, policy) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.learning_rate.assign(0.0001)\n",
    "model.fit({'input':samples}, {'policy': policy, 'value':value},verbose=2, epochs=5000)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1=\"gen\"+str(1).zfill(4)\n",
    "#model=tf.keras.models.load_model(g1+'.h5', custom_objects={\"policy_loss\": policy_loss})\n",
    "#Saving\n",
    "print('Save Model '+g1+'.h5')\n",
    "model.save(g1+'.h5')\n",
    "SaveModel(model,g1+\".w32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salida=model.predict(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=11\n",
    "print(salida[1][K])\n",
    "print(policy[K])\n",
    "print(salida[0][K][0])\n",
    "print(value[K][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(gen_name+'.h5')\n",
    "SaveModel(model,gen_name+\".w32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitplay_process=os.path.join(\".\",\"CGZero\")+\" pitplay \"+PIT_PARAM_THREAD+\" \"+gen_name+\" \"+PIT_PARAM_MCTS+\" \"+gen_best2+\" \"+PIT_PARAM_MCTS\n",
    "print('subprocess.run('+pitplay_process+', shell=True)')\n",
    "subprocess.run(pitplay_process, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitplay_process=os.path.join(\".\",\"CGZero\")+\" pitplay \"+PIT_PARAM_THREAD+\" \"+gen_name+\" \"+PIT_PARAM_MCTS+\" \"+gen_best2+\" \"+PIT_PARAM_MCTS\n",
    "print('subprocess.run('+pitplay_process+', shell=True)')\n",
    "subprocess.run(pitplay_process, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfplay_process=os.path.join(\".\",\"CGZero\")+\" selfplay \"+f\"{THREADS} 500 \"+gen_best1+\" \"+TRAIN_PARAMS+\" \"+gen_best2+\" \"+TRAIN_PARAMS\n",
    "print(selfplay_process)\n",
    "subprocess.run(selfplay_process, shell=True)\n",
    "\n",
    "selfplay_process=os.path.join(\".\",\"CGZero\")+\" selfplay \"+f\"{THREADS} 500 \"+gen_best2+\" \"+TRAIN_PARAMS+\" \"+gen_best2+\" \"+TRAIN_PARAMS\n",
    "print(selfplay_process)\n",
    "subprocess.run(selfplay_process, shell=True)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = np.fromfile(SAMPLES_FILE, dtype=np.float32)\n",
    "csv_data=np.reshape(csv_data, (-1,INPUT_SIZE+POLICY_SIZE+2))\n",
    "cut_index = [(csv_data.shape)[1]-POLICY_SIZE-2, (csv_data.shape)[1]-2,(csv_data.shape)[1]-1]\n",
    "samples,policy,value,countVisits=np.split(csv_data, cut_index,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(countVisits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_1=\"gen0242\"\n",
    "nn_2=\"gen0239\"\n",
    "no_matches=300\n",
    "custom=f\"./CGZero selfplay {THREADS} {no_matches} {nn_1} 4.0 0.0034086042277349836 4.0 2000 0.24 1.3 0.0 0.05 0.9 0.1 20 5 {nn_2} 4.0 0.0034086042277349836 4.0 2000 0.24 1.3 0.0 0.05 0.9 0.1 20 5\"\n",
    "print(custom)\n",
    "subprocess.run(custom, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "AlphaZero.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
