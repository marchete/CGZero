{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### INITIALIZATION #####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2064,
     "status": "ok",
     "timestamp": 1622443062342,
     "user": {
      "displayName": "Marchete",
      "photoUrl": "",
      "userId": "00000078181859324609"
     },
     "user_tz": -120
    },
    "id": "xJqDIgorJBbK",
    "outputId": "91ab953e-8c83-4a48-b029-5418aa1261ac"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "# Install TensorFlow\n",
    "\n",
    "import tensorflow as tf\n",
    "#THREADS=3\n",
    "#tf.config.threading.set_intra_op_parallelism_threads(THREADS)\n",
    "#tf.config.threading.set_inter_op_parallelism_threads(THREADS)\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import base64\n",
    "import sys\n",
    "import glob, os\n",
    "import re\n",
    "import subprocess\n",
    "import datetime\n",
    "import queue\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\"Python:\"+sys.version)\n",
    "print(\"TF:\"+tf.__version__)\n",
    "print(\"GPU:\"+str(tf.test.is_gpu_available())+\" CUDA:\"+str(tf.test.is_built_with_cuda()))\n",
    "#print(device_lib.list_local_devices())\n",
    "import os\n",
    "if not os.path.exists('/run/shm/traindata'):\n",
    "    os.makedirs('/run/shm/traindata')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crkTUlVwJBbO"
   },
   "outputs": [],
   "source": [
    "######################################### HYPERPARAMETERS AND CONFIGS #####################################\n",
    "\n",
    "#Don't touch INPUT_SIZE UNLESS YOU ARE CHANGING THE NN INPUTS DEFINITION. POLICY_SIZE IS THE NUMBER OF LEGAL MOVES ON THE GAME.\n",
    "#INPUT_SIZE must match what CGZero.cpp says in function:\n",
    "# int  _Game::getInputDimensions() {...}\n",
    "INPUT_SIZE=6*2*24+2*27\n",
    "#The same than CGZero.cpp function: int getPolicySize() {...}\n",
    "POLICY_SIZE=6\n",
    "\n",
    "\n",
    "#HYPERPARAMETERS. You MUST assign them a value.\n",
    "#DISCLAIMER: I have ABSOLUTELY no idea what are good hyperparameters. Most AZ documentation is confusing or even contradictory.\n",
    "MATCHES_PER_GENERATION=   #Something around 400 and 3000\n",
    "PIT_MATCHES=  #I have about 400, too little matches will not be enough to know if a candidate is best.\n",
    "THREADS=6  #CPU dependant, on a Core i7-8700K I can set it to 9. With 6 I can use the PC for other purposes, like gaming.\n",
    "TRAINING_POOL_SIZE=      #At least 500000, but can be millions.  500k-2M I'd say\n",
    "                         #If it's too small there won't be enough samples.\n",
    "                         #If it's too big it will use very old samples that might not have good values to learn.\n",
    "TRAINING_SUBSET_SIZE=   #I noticed better learning with more samples. 200k is not enough, 500k and up seemed a better value\n",
    "K_BATCH_SIZE= #I have no idea what to use, I used 64 in last tests, but on others I put bigger batch sizes, like 512 or 1024\n",
    "K_ITERATIONS= #Unused, this was for the minibatch approach from original Alphazero. There is commented code in the training loop about that\n",
    "K_EPOCH=  #Number of full learning passes the Tensorflow will do with the samples subset, from 1 to 60 I guess. If the learning process is slow I put a lower value.\n",
    "\n",
    "K_WEIGHT_POLICY=1.0  #Give more importance to Policy losses\n",
    "K_WEIGHT_VALUE=1.0   \n",
    "\n",
    "WINRATE_ACCEPTED=55.6 #Or 55.0, what you prefer\n",
    "\n",
    "K_LEARNING_RATE=0.001 #I imagine that at some point it must be lowered, I have no idea exactly when.\n",
    "K_MOMENTUM=0.87 #Some hyperparameter for the training part. See https://distill.pub/2017/momentum/ \n",
    "\n",
    "#Parameters that controls how the endgame score is backpropagated to the samples.\n",
    "#https://medium.com/oracledevs/lessons-from-alphazero-part-4-improving-the-training-target-6efba2e71628\n",
    "#For each sample the value I use is= B * ENDGAME_SCORE + (1.0 - B)*MeanScore\n",
    "#Where B is  B = PROPAGATE_BASE + PROPAGATE_INC * (seenMoves / totalMovesInReplayBuffer)\n",
    "#  and MeanScore is the MCTS mean value (Q).\n",
    "#Alphazero uses z: PROPAGATE_BASE = 1.0 and PROPAGATE_INC = 0.0\n",
    "#Others use q: PROPAGATE_BASE = 0.0 and PROPAGATE_INC = 0.0\n",
    "#I'm using a mix of z and q, something like PROPAGATE_BASE = 0.45 and PROPAGATE_INC = 1.0-PROPAGATE_BASE\n",
    "PROPAGATE_BASE=   # 0.0 to 1.0. Percentage of endgame Score (-1 for loss and +1 for win) that the sample on turn 0 will have.\n",
    "PROPAGATE_INC=1.0-PROPAGATE_BASE #Percentage of endgame score at end.\n",
    "\n",
    "#I've used the same idea for the policy part. I dislike the \"temperature\" thing on Alphazero, I just tweak samples to be\n",
    "# POLICY = B * VisitsPOLICY + (1.0 - B)*OneHotPOLICY\n",
    "#VisitsPOLICY is calculated by dividing visitsChildren/visitsParent, so you have a 100% distribution\n",
    "#OneHotPOLICY is all zeros except the selected move that it's a 1.0. It's also a 100% distribution\n",
    "POLICY_BACKP_FIRST=10 #; //Similarly , but with percentage of turns, first 10% of turns doesn't have any \"temperature\",\n",
    "POLICY_BACKP_LAST=5 #; //from (100-5=95%) I linearly sharpen policy to get only the best move, a one-hot policy\n",
    "\n",
    "#TRAINING PARAMETERS\n",
    "#I literally have no idea what's going on with the cpuct hyperparameter. It controls the exploration part on the MCTS search.\n",
    "#But the problem is that I don't see any consensus about what's the best way to control it.\n",
    "#https://lczero.org/blog/2018/12/alphazero-paper-and-lc0-v0191/   Cpuct is not a constant!!!!!!!\n",
    "#https://medium.com/oracledevs/lessons-from-alpha-zero-part-6-hyperparameter-tuning-b1cfcbe4ca9a  Trial and error way\n",
    "TRAIN_CPUCT_MIN=  #Maybe something between 2.0 and 3.0, but I'm clueless. 1.0 was \"good\" (>70% winrate) on pit, but not that good when submitted to CG\n",
    "TRAIN_CPUCT_INC=0.00\n",
    "TRAIN_CPUCT_MAX=TRAIN_CPUCT_MIN\n",
    "\n",
    "TRAIN_NOISE_DIR_EPSILON= #it gives diversity, something between 0.10 and 0.30 can be good for selfplay\n",
    "TRAIN_NOISE_DIR_ALPHA= #More than 1.0 always, maybe in the 1.0-1.6 range.\n",
    "TRAIN_NOISE_DIR_DECAY=0.0 #Reduce noise each turn. Maybe it's not good to have much noise at endgame.\n",
    "TRAIN_MCTS_ITER= #At least 800, but the more, the better quality of predictions but slower sample generation. 2k or 4k works fine too\n",
    "TRAIN_NOISE_RANDOM= #Simplistic random noise to NN value. 0.03 means a  randomFloat(0.97,1.03)*NNvalue\n",
    "#Don't touch that\n",
    "TRAIN_PARAMS = f\"{TRAIN_CPUCT_MIN} {TRAIN_CPUCT_INC} {TRAIN_CPUCT_MAX} {TRAIN_MCTS_ITER} {TRAIN_NOISE_DIR_EPSILON} {TRAIN_NOISE_DIR_ALPHA} {TRAIN_NOISE_DIR_DECAY} {TRAIN_NOISE_RANDOM} {PROPAGATE_BASE} {PROPAGATE_INC} {POLICY_BACKP_FIRST} {POLICY_BACKP_LAST}\"\n",
    "\n",
    "#PIT PARAMETERS\n",
    "PIT_CPUCT_MIN=TRAIN_CPUCT_MIN\n",
    "PIT_CPUCT_INC=0.00\n",
    "PIT_CPUCT_MAX=PIT_CPUCT_MIN\n",
    "\n",
    "PIT_NOISE_DIR_EPSILON=0.03 #Use much lower dirichlet noise than in selfplay. We need diversity but not noise.\n",
    "PIT_NOISE_DIR_ALPHA=1.0\n",
    "PIT_NOISE_DIR_DECAY=0.0006\n",
    "PIT_MCTS_ITER=TRAIN_MCTS_ITER\n",
    "PIT_NOISE_RANDOM=0.02\n",
    "#Don't touch that\n",
    "PIT_PARAM_THREAD= f\"{THREADS} {PIT_MATCHES}\"\n",
    "PIT_PARAM_MCTS = f\"{PIT_CPUCT_MIN} {PIT_CPUCT_INC} {PIT_CPUCT_MAX} {PIT_MCTS_ITER} {PIT_NOISE_DIR_EPSILON} {PIT_NOISE_DIR_ALPHA} {PIT_NOISE_DIR_DECAY} {PIT_NOISE_RANDOM} {PROPAGATE_BASE} {PROPAGATE_INC} {POLICY_BACKP_FIRST} {POLICY_BACKP_LAST}\"\n",
    "SAMPLES_FILE=os.path.join(\".\",\"traindata\",\"samples.dat\")\n",
    "sampler_process=os.path.join(\".\",\"NNSampler\")+\" \"+os.path.join(\".\",\"traindata\")+\" Replay.*.dat \"+SAMPLES_FILE+\" \"+str(TRAINING_POOL_SIZE)+\" \"+str(TRAINING_SUBSET_SIZE)+\" \"+str(INPUT_SIZE)+\" \"+str(1+POLICY_SIZE)+\" 1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### FUNCTIONS USED FOR TRAINING #####################################\n",
    "def adjustLR():\n",
    "    if generation == 0:\n",
    "        return 8*K_LEARNING_RATE\n",
    "    elif generation < 5:\n",
    "        return 6*K_LEARNING_RATE\n",
    "    elif generation < 10:\n",
    "        return 2*K_LEARNING_RATE\n",
    "    else:\n",
    "        return K_LEARNING_RATE\n",
    "\n",
    "#Save weights, non trainable layers must be named \"IGNORE_*\"\n",
    "def SaveModel(my_model,fileSTR):\n",
    "    totalbytes=0\n",
    "    data=[]\n",
    "    Wmodel = open(\"./\"+fileSTR, \"wb\")\n",
    "    for x in my_model.weights:\n",
    "        if (\"IGNORE_\" in x.name):\n",
    "            #print(\"Ignoring layer \"+x.name)\n",
    "            continue\n",
    "        nn = x.numpy()\n",
    "        T = nn\n",
    "        v = np.ndarray.tobytes(T)\n",
    "        Wmodel.write(bytearray(v))\n",
    "        totalbytes+=len(v)\n",
    "        data.append(base64.b64encode(v).decode(\"utf-8\"))\n",
    "    Wmodel.close()\n",
    "def readWinrate(candidatefile,bestfile):\n",
    "    files=sorted(glob.glob(os.path.join('pitresults','Pit_'+candidatefile+'_'+bestfile+'_*.txt')),reverse=True)\n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            strvalue=f.read().strip()\n",
    "        return float(strvalue)\n",
    "    return -1.0\n",
    "def readAndDeleteWinrate(candidatefile,bestfile):\n",
    "    files=sorted(glob.glob(os.path.join('pitresults','Pit_'+candidatefile+'_'+bestfile+'_*.txt')),reverse=True)\n",
    "    valor=-1.0\n",
    "    for file in files:\n",
    "        if (valor == -1.0):\n",
    "            with open(file, 'r') as f:\n",
    "                valor=float(f.read().strip())\n",
    "        os.remove(file)\n",
    "    return valor\n",
    "\n",
    "#LR_decay = tf.keras.callbacks.LearningRateScheduler(tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-3,decay_steps=20000,decay_rate=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### MODEL DEFINITION #####################################\n",
    "## Must match the model you have in CGZero.cpp, in function\n",
    "##   Model _Game::CreateNNModel(bool activeSoftMax) {"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEr5GjiJJBbP"
   },
   "outputs": [],
   "source": [
    "#It's INCOMPLETE, YOU MUST CREATE YOUR OWN MODEL!!\n",
    "#Order is important! You must take into account that this model and C++ counterpart must be synchronized, so layer orders must be the same between languages.\n",
    "\n",
    "#Input layer\n",
    "inputs =  tf.keras.Input(shape=(INPUT_SIZE,), name='input')\n",
    "#Common part of the Model, both policy and value use these layers\n",
    "x = tf.keras.layers.Dense(TODOTODOTODOT,activation='relu',name='Dense1')(inputs)\n",
    "#x = tf.keras.layers.Dense(TODOTODOTODOT,activation='relu')(x)\n",
    "\n",
    "#Split part 1, P1 layers are for the policy part. If you don't want extra layers for p1, I guess you can do p1=x\n",
    "p1 = tf.keras.layers.Dense(TODOTODOTODOT,activation='relu',name='p1')(x)\n",
    "#p1 = tf.keras.layers.Dense(TODOTODOTODOT,activation='relu')(p1)\n",
    "\n",
    "#Split part, v1 layers are for the value part. If you don't want extra layers for v1, I guess you can do v1=x\n",
    "v1 = tf.keras.layers.Dense(TODOTODOTODOT,activation='relu',name='v1')(x)\n",
    "#v1 = tf.keras.layers.Dense(TODOTODOTODOT,activation='relu')(v1)\n",
    "#Output layers, don't touch them if you don't know what are you doing.\n",
    "value = tf.keras.layers.Dense(1, activation='tanh',name='value')(v1)\n",
    "policy = tf.keras.layers.Dense(POLICY_SIZE, activation='softmax',name='policy')(p1)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=[value, policy])\n",
    "#Others use Adam as optimizer, I just used SGD because I'm clueless and I saw some others using SGD.\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=K_LEARNING_RATE, momentum=K_MOMENTUM)\n",
    "#Keep a file with losses history\n",
    "csv_logger = tf.keras.callbacks.CSVLogger('gen_train.log',append=True)\n",
    "\n",
    "model.compile(loss={'value': 'mean_squared_error',\n",
    "                    'policy':tf.keras.losses.KLD  },\n",
    "              optimizer=opt,\n",
    "              loss_weights = {'value':K_WEIGHT_VALUE,\n",
    "                              'policy':K_WEIGHT_POLICY}\n",
    "              ,metrics={'value':'mean_absolute_percentage_error',\n",
    "                       'policy': 'categorical_accuracy' }\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kbz3qqCMJBbQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('gen0000.h5'):\n",
    "    model.save('gen0000.h5')\n",
    "    SaveModel(model,\"gen0000.w32\")\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### RESUME TRAINING #####################################\n",
    "# load best models, load last generation number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1I1ex_D8JBbS",
    "outputId": "fd3d4b2c-00dc-44a2-d88b-77ba3f51b487"
   },
   "outputs": [],
   "source": [
    "generation=0\n",
    "gen_name=\"gen\"+str(generation).zfill(4)\n",
    "if os.path.exists('generation.txt'):\n",
    "    with open('generation.txt', 'r') as f:\n",
    "        generation = int(f.read().strip())\n",
    "        gen_name=\"gen\"+str(generation).zfill(4)\n",
    "        print(\"Generation is :\"+gen_name+\" \"+str(generation))        \n",
    "        if (generation > 0):\n",
    "            #model =  tf.keras.models.load_model(gen_name+'.h5', custom_objects={\"policy_loss\": policy_loss})\n",
    "            model =  tf.keras.models.load_model(gen_name+'.h5')\n",
    "if not os.path.exists(gen_name+\".w32\"):\n",
    "    SaveModel(model,gen_name+\".w32\")\n",
    "    model.save(gen_name+'.h5')\n",
    "gen_best1=gen_name\n",
    "if os.path.exists('gen_best1.txt'):\n",
    "    with open('gen_best1.txt', 'r') as f:\n",
    "        gen_best1 = f.read().strip()\n",
    "gen_best2=gen_name\n",
    "if os.path.exists('gen_best2.txt'):\n",
    "    with open('gen_best2.txt', 'r') as f:\n",
    "        gen_best2 = f.read().strip()        \n",
    "model=tf.keras.models.load_model(gen_name+'.h5')\n",
    "print(\"Best Model1:\"+gen_best1+\" + \"+gen_best2+\". Current generation:\"+gen_name+\" loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### MAIN TRAINING LOOP #####################################\n",
    "# 1- Creates self plays between best models (and also some random generation to give diversity)\n",
    "# 2- Creates a random sample dataset from selfplay\n",
    "# 3- Train the current model\n",
    "# 4- Save candidate as a new generation (useful to resume later, or return back to a previous train state)\n",
    "# 5- Pit play vs best1 and best2, promote candidate as best if winrate is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zULZ7RjSJBbT",
    "outputId": "93c0f940-cecf-41cf-ae95-e337af017d60",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clear_output(wait=True)\n",
    "try:\n",
    "    pit_winrate2\n",
    "except NameError:\n",
    "    pit_winrate2=70.0\n",
    "try:\n",
    "    pit_winrate\n",
    "except NameError:\n",
    "    pit_winrate=max(0,100.0-pit_winrate2)\n",
    "    \n",
    "while True:\n",
    "    model.optimizer.learning_rate.assign(adjustLR())\n",
    "    if (generation % 10 == 0):\n",
    "        clear_output(wait=True)\n",
    "    samplescount=0\n",
    "    #if (generation == 0):\n",
    "     #   csv_data = np.fromfile(SAMPLES_FILE, dtype=np.float32)\n",
    "      #  csv_data=np.reshape(csv_data, (-1,INPUT_SIZE+POLICY_SIZE+2))\n",
    "       # samplescount =(csv_data.shape)[0]\n",
    "\n",
    "    #Remove generation 0 data once we have samples with more quality\n",
    "    if (generation > 4 and generation < 6):\n",
    "        gen0=\"gen\"+str(0).zfill(4)\n",
    "        remove0=glob.glob(os.path.join('traindata','Replay_'+'*'+gen0+\"*\"+'.dat'))\n",
    "        for filePath in remove0:\n",
    "            try:\n",
    "                os.remove(filePath)\n",
    "            except:\n",
    "                pass\n",
    "    # 1- Creates self plays between best models (and also some random generation to give diversity)\n",
    "    # The code will repeat until it has enough samples (only after generation 10, lower ones are of bad quality).\n",
    "    while samplescount < TRAINING_SUBSET_SIZE/3:\n",
    "        if (generation<=1):\n",
    "            random_enemy=gen_name\n",
    "        else:\n",
    "            random_enemy=\"gen\"+str(random.randint(max(1,generation-5), generation)).zfill(4)\n",
    "        \n",
    "        if (generation==0 and (len(glob.glob(os.path.join('traindata','Replay_'+'*'+gen_name+\"vs\"+gen_name+'.dat')))>0)):\n",
    "            print('Replay_'+'*'+gen_name+'.txt already exists') \n",
    "        else:\n",
    "            pFirst=max(0.2,0.9*(1.0-(pit_winrate/(pit_winrate+pit_winrate2))))\n",
    "            pSecond=0.9-pFirst\n",
    "            print(f\" **** Doing samples. Count:{samplescount}. pBest1:{100.0*pFirst}% p2:{100.0*pSecond}%  {pit_winrate} {pit_winrate2}\")\n",
    "            p70=int(pFirst*MATCHES_PER_GENERATION)\n",
    "            p20=int(pSecond*MATCHES_PER_GENERATION)\n",
    "            p5=MATCHES_PER_GENERATION-p70-p20\n",
    "            selfplay_process=os.path.join(\".\",\"CGZero\")+\" selfplay \"+f\"{THREADS} {p70} \"+gen_best1+\" \"+TRAIN_PARAMS+\" \"+gen_best1+\" \"+TRAIN_PARAMS\n",
    "            print(selfplay_process)\n",
    "            subprocess.run(selfplay_process, shell=True)\n",
    "            if (gen_best1 != gen_best2):\n",
    "                selfplay_process=os.path.join(\".\",\"CGZero\")+\" selfplay \"+f\"{THREADS} {p20} \"+gen_best1+\" \"+TRAIN_PARAMS+\" \"+gen_best2+\" \"+TRAIN_PARAMS\n",
    "                print(selfplay_process)\n",
    "                subprocess.run(selfplay_process, shell=True)\n",
    "                if (p70 >= p20):\n",
    "                    A=gen_best1 if (gen_best1 >= random_enemy) else random_enemy\n",
    "                    B=gen_best1 if (A == random_enemy) else random_enemy\n",
    "                else:\n",
    "                    A=gen_best2 if (gen_best2 >= random_enemy) else random_enemy\n",
    "                    B=gen_best2 if (A == random_enemy) else random_enemy\n",
    "                selfplay_process=os.path.join(\".\",\"CGZero\")+\" selfplay \"+f\"{THREADS} {p5} \"+A+\" \"+TRAIN_PARAMS+\" \"+B+\" \"+TRAIN_PARAMS\n",
    "                print(selfplay_process)\n",
    "                subprocess.run(selfplay_process, shell=True)\n",
    "        print('Reading training data')\n",
    "        # 2- Creates a random sample dataset from selfplay\n",
    "        print(sampler_process)\n",
    "        subprocess.run(sampler_process, shell=True)\n",
    "        csv_data = np.fromfile(SAMPLES_FILE, dtype=np.float32)\n",
    "        csv_data=np.reshape(csv_data, (-1,INPUT_SIZE+POLICY_SIZE+2))\n",
    "        samplescount =(csv_data.shape)[0]\n",
    "        if (generation < 10):\n",
    "            break\n",
    "    #Cut samples to inputs , policy , value , countVisits\n",
    "    np.random.shuffle(csv_data)\n",
    "    cut_index = [(csv_data.shape)[1]-POLICY_SIZE-2, (csv_data.shape)[1]-2,(csv_data.shape)[1]-1]\n",
    "    samples,policy,value,countVisits=np.split(csv_data, cut_index,axis=1)\n",
    "    \n",
    "    mask= np.where(policy < 0, -999999999.99, policy)\n",
    "    policy= np.where(policy < 0, 0, policy) \n",
    "  \n",
    "    # 3- Train the current model\n",
    "    ####Minibatches learning: This is for doing minibatches, but I prefer to simply feed all the samples subset.\n",
    "    #for loop in range(K_ITERATIONS):\n",
    "    #    print(\"Batch \"+str(loop)+\":\",end='')\n",
    "    #    indices = np.random.choice(value.shape[0], K_BATCH_SIZE, replace=False)\n",
    "    #    S=samples[indices]\n",
    "    #    P=policy[indices]\n",
    "    #    V=value[indices]\n",
    "    #    #model.optimizer.learning_rate.assign(learning_rate_scheduler(loop, 0.0))\n",
    "    #    #model.optimizer.learning_rate.assign(K_LEARNING_RATE)\n",
    "    #    model.fit({'input':S}, {'policy': P, 'value':V},verbose=2, epochs=K_EPOCH,callbacks=[csv_logger],batch_size=int(K_BATCH_SIZE/4))\n",
    "    \n",
    "    ####Simple learning, just learn from all the subset. If you activate the former, disable this line\n",
    "    model.fit({'input':samples}, {'policy': policy, 'value':value},verbose=2, epochs=min(generation+1,K_EPOCH),callbacks=[csv_logger],batch_size=int(K_BATCH_SIZE/4))    \n",
    "    \n",
    "    #new generation\n",
    "    print('New generation '+gen_name+' -> '+\"gen\"+str(generation+1).zfill(4))\n",
    "    generation=generation+1\n",
    "    gen_name=\"gen\"+str(generation).zfill(4)\n",
    "    # 4- Save candidate as a new generation (useful to resume later, or return back to a previous train state)\n",
    "    print('Save Model '+gen_name+'.h5')\n",
    "    model.save(gen_name+'.h5')\n",
    "    SaveModel(model,gen_name+\".w32\")\n",
    "    with open('generation.txt', 'w') as f:\n",
    "        f.write(str(generation))    \n",
    "        \n",
    "    # 5- Pit play vs best1 and best2, promote candidate as best if winrate is good\n",
    "    pitplay_process=os.path.join(\".\",\"CGZero\")+\" pitplay \"+PIT_PARAM_THREAD+\" \"+gen_name+\" \"+PIT_PARAM_MCTS+\" \"+gen_best1+\" \"+PIT_PARAM_MCTS\n",
    "    print('subprocess.run('+pitplay_process+', shell=True)')\n",
    "    subprocess.run(pitplay_process, shell=True)\n",
    "    pit_winrate=readWinrate(gen_name,gen_best1)\n",
    "    if gen_best1 == gen_best2:\n",
    "        pit_winrate2=pit_winrate\n",
    "    else:\n",
    "        pitplay_process=os.path.join(\".\",\"CGZero\")+\" pitplay \"+PIT_PARAM_THREAD+\" \"+gen_name+\" \"+PIT_PARAM_MCTS+\" \"+gen_best2+\" \"+PIT_PARAM_MCTS\n",
    "        subprocess.run(pitplay_process, shell=True)\n",
    "        pit_winrate2=readWinrate(gen_name,gen_best2)\n",
    "    print('Winrate '+str(pit_winrate)+' '+str(pit_winrate2))\n",
    "    #Check if it's a new best, update bests\n",
    "    if (pit_winrate>=WINRATE_ACCEPTED):\n",
    "        print(\"New best:\"+gen_name+\" vs \"+gen_best1+\": Winrate1:\"+str(pit_winrate)+\"%\")\n",
    "        print(\"        :\"+gen_name+\" vs \"+gen_best2+\": Winrate2:\"+str(pit_winrate2)+\"%\")\n",
    "        tmpgenbest1=gen_best1\n",
    "        gen_best1=gen_name\n",
    "        with open('gen_best1.txt', 'w') as f:\n",
    "            f.write(gen_best1)\n",
    "        if (pit_winrate2>=50.0 and tmpgenbest1 != gen_best2):\n",
    "            gen_best2=tmpgenbest1\n",
    "            with open('gen_best2.txt', 'w') as f:\n",
    "                f.write(gen_best2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### AUXILIARY TOOLBOX #####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###################### MODEL VALIDATION ###########################\n",
    "#Run this script, then copy the output to C++ main() function to print a Model prediction in C++. Both must be the same\n",
    "print(\"//########################## ZERO TEST\")\n",
    "test_zero=np.zeros([1,INPUT_SIZE], dtype=np.float32)\n",
    "predictions = model.predict(test_zero)\n",
    "model.save('validate.h5')\n",
    "SaveModel(model,\"validate.w32\")\n",
    "print(\"//Use the following code in C++ to validate Model consistency\")\n",
    "print('ValidateModel(\"validate.w32\",\tR\"('+f\"{' '.join(map(str, test_zero[0]))})\\\");\")\n",
    "\n",
    "print(\"//Value Predicted:\",end='')\n",
    "print(predictions[0][0])\n",
    "\n",
    "print(\"//Policy Predicted:\",end='')\n",
    "print(predictions[1][0])\n",
    "print(\"//Ensure that Value and Policy values are the same\")\n",
    "print(\"//########################## RANDOM TEST\")\n",
    "test_random=np.random.rand(1,INPUT_SIZE)\n",
    "predictions = model.predict(test_random)\n",
    "print(\"//Use the following code in C++ to validate Model consistency\")\n",
    "print('ValidateModel(\"validate.w32\",\tR\"('+f\"{' '.join(map(str, test_random[0]))})\\\");\")\n",
    "\n",
    "print(\"//Value Predicted:\",end='')\n",
    "print(predictions[0][0])\n",
    "\n",
    "print(\"//Policy Predicted:\",end='')\n",
    "print(predictions[1][0])\n",
    "print(\"//Ensure that Value and Policy values are the same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search a new CPUCT, it pits the same NN model with different coeffs, and keep a report on best_upct.txt file\n",
    "import shutil\n",
    "shutil.copyfile(gen_best1+'.w32', 'A.w32')\n",
    "shutil.copyfile(gen_best2+'.w32', 'B.w32')\n",
    "#cpupt optimizer\n",
    "BEST_CPUCT_MAX=1.00+random.random()*5.0    \n",
    "BEST_CPUCT_MIN=(BEST_CPUCT_MAX-0.40)*random.random()+0.40\n",
    "BEST_CPUCT_INC=0.001+0.035*random.random()\n",
    "TEST_PARAM_THREAD= f\"{THREADS} 150\"\n",
    "try:\n",
    "    bestWinrate\n",
    "except NameError:\n",
    "    bestWinrate=0.0\n",
    "with open('best_upct.txt', 'a') as f:\n",
    "    f.write(\"Searching on best:\"+gen_best1+\"\\n\")\n",
    "for upt in range(100):\n",
    "    TEST_CPUCT_INC=0.001+0.035*random.random()\n",
    "    TEST_CPUCT_MAX=1.00+random.random()*5.0    \n",
    "    TEST_CPUCT_MIN=(TEST_CPUCT_MAX-1.00)*random.random()+1.00    \n",
    "    if (random.random() < 0.5):\n",
    "        TEST_CPUCT_INC=-TEST_CPUCT_INC\n",
    "        T=TEST_CPUCT_MIN\n",
    "        TEST_CPUCT_MIN=TEST_CPUCT_MAX\n",
    "        TEST_CPUCT_MAX=T\n",
    "    #pit\n",
    "    #TEST_CPUCT_MAX=1.00+random.random()*5.0    \n",
    "    #TEST_CPUCT_MIN=(TEST_CPUCT_MAX-1.00)*random.random()+1.00\n",
    "    #TEST_CPUCT_INC=0.001+0.035*random.random()\n",
    "    TEST_CPUCT_MIN=3.0-0.30+2.0*0.30*random.random()\n",
    "    TEST_CPUCT_MAX=TEST_CPUCT_MIN+2.0*0.05*random.random()\n",
    "    TEST_CPUCT_INC=0.00360-0.00080+2.0*0.00080*random.random()\n",
    "    \n",
    "\n",
    "    UPT_CC = f\"{TEST_CPUCT_MIN} {TEST_CPUCT_INC} {TEST_CPUCT_MAX} {PIT_MCTS_ITER} {PIT_NOISE_DIR_EPSILON} {PIT_NOISE_DIR_ALPHA} {PIT_NOISE_DIR_DECAY} {PIT_NOISE_RANDOM} {PROPAGATE_BASE} {PROPAGATE_INC} {POLICY_BACKP_FIRST} {POLICY_BACKP_LAST}\"    \n",
    "    pitplay_process=os.path.join(\".\",\"CGZero\")+\" pitplay \"+TEST_PARAM_THREAD+\" A \"+UPT_CC+\" A \"+PIT_PARAM_MCTS\n",
    "    print('subprocess.run('+pitplay_process+', shell=True)')\n",
    "    subprocess.run(pitplay_process, shell=True)\n",
    "    pit_winrate=readAndDeleteWinrate(\"A\",\"A\")\n",
    "    if (pit_winrate > 50.0 and pit_winrate > bestWinrate-5.0):\n",
    "        pitplay_process=os.path.join(\".\",\"CGZero\")+\" pitplay \"+TEST_PARAM_THREAD+\" A \"+UPT_CC+\" B \"+PIT_PARAM_MCTS\n",
    "        subprocess.run(pitplay_process, shell=True)\n",
    "        pit_winrate+=readAndDeleteWinrate(\"A\",\"B\")\n",
    "        pit_winrate*=0.5\n",
    "    with open('best_upct.txt', 'a') as f:\n",
    "        f.write(f\"{pit_winrate} | {TEST_CPUCT_MIN} {TEST_CPUCT_INC} {TEST_CPUCT_MAX} \"+\"\\n\")    \n",
    "    print('Winrate '+str(pit_winrate))\n",
    "    if (pit_winrate > bestWinrate):\n",
    "        bestWinrate = pit_winrate\n",
    "        BEST_CPUCT_MAX=TEST_CPUCT_MAX   \n",
    "        BEST_CPUCT_MIN=TEST_CPUCT_MIN\n",
    "        BEST_CPUCT_INC=TEST_CPUCT_INC\n",
    "        print(f\"New best {TEST_CPUCT_MIN} {TEST_CPUCT_INC} {TEST_CPUCT_MAX} {bestWinrate}% \")\n",
    "        with open('best_upct.txt', 'a') as f:\n",
    "            f.write(f\"New best {bestWinrate}% | {TEST_CPUCT_MIN} {TEST_CPUCT_INC} {TEST_CPUCT_MAX}\"+\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = np.fromfile(SAMPLES_FILE, dtype=np.float32)\n",
    "csv_data=np.reshape(csv_data, (-1,INPUT_SIZE+POLICY_SIZE+2))\n",
    "cut_index = [(csv_data.shape)[1]-POLICY_SIZE-2, (csv_data.shape)[1]-2,(csv_data.shape)[1]-1]\n",
    "samples,policy,value=np.split(csv_data, cut_index,axis=1)\n",
    "\n",
    "mask= np.where(policy < 0, -999999999.99, policy)\n",
    "policy= np.where(policy < 0, 0, policy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.learning_rate.assign(0.003)\n",
    "model.fit({'input':samples}, {'policy': policy, 'value':value},verbose=2, epochs=int(20),callbacks=[csv_logger],batch_size=64)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(gen_name+'.h5')\n",
    "SaveModel(model,gen_name+\".w32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitplay_process=os.path.join(\".\",\"CGZero\")+\" pitplay \"+PIT_PARAM_THREAD+\" \"+gen_name+\" \"+PIT_PARAM_MCTS+\" \"+gen_best2+\" \"+PIT_PARAM_MCTS\n",
    "print('subprocess.run('+pitplay_process+', shell=True)')\n",
    "subprocess.run(pitplay_process, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfplay_process=os.path.join(\".\",\"CGZero\")+\" selfplay \"+f\"{THREADS} 500 \"+gen_best1+\" \"+TRAIN_PARAMS+\" \"+gen_best2+\" \"+TRAIN_PARAMS\n",
    "print(selfplay_process)\n",
    "subprocess.run(selfplay_process, shell=True)\n",
    "\n",
    "selfplay_process=os.path.join(\".\",\"CGZero\")+\" selfplay \"+f\"{THREADS} 500 \"+gen_best2+\" \"+TRAIN_PARAMS+\" \"+gen_best2+\" \"+TRAIN_PARAMS\n",
    "print(selfplay_process)\n",
    "subprocess.run(selfplay_process, shell=True)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = np.fromfile(SAMPLES_FILE, dtype=np.float32)\n",
    "csv_data=np.reshape(csv_data, (-1,INPUT_SIZE+POLICY_SIZE+2))\n",
    "cut_index = [(csv_data.shape)[1]-POLICY_SIZE-2, (csv_data.shape)[1]-2,(csv_data.shape)[1]-1]\n",
    "samples,policy,value,countVisits=np.split(csv_data, cut_index,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_1=\"gen0242\"\n",
    "nn_2=\"gen0239\"\n",
    "no_matches=300\n",
    "custom=f\"./CGZero selfplay {THREADS} {no_matches} {nn_1} 4.0 0.0034086042277349836 4.0 2000 0.24 1.3 0.0 0.05 0.9 0.1 20 5 {nn_2} 4.0 0.0034086042277349836 4.0 2000 0.24 1.3 0.0 0.05 0.9 0.1 20 5\"\n",
    "print(custom)\n",
    "subprocess.run(custom, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.models.load_model(\"gen\"+str(36).zfill(4)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitplay_process=os.path.join(\".\",\"CGZero\")+\" pitplay \"+PIT_PARAM_THREAD+\" \"+gen_name+\" \"+PIT_PARAM_MCTS+\" \"+gen_best2+\" \"+PIT_PARAM_MCTS\n",
    "print('subprocess.run('+pitplay_process+', shell=True)')\n",
    "subprocess.run(pitplay_process, shell=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "AlphaZero.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
